CHANGE THEME OF GRAPHS -- WHITE BACKGROUND 


library 
```{r}
library(readtext) #Needed for readtext
library(tidyverse)
library(tm) #clean
library(tidytext) #unnest
library(textstem) #lemmatize
```

BY AUTHOR POSITIONALITY
```{r}
#white male 
#read
wm_df <- readtext("data/white_male_auth", cache=FALSE)
#remove .txt
wm_df$doc_id<-gsub(".txt", "", paste(wm_df$doc_id))
#still nested for +-10
wm_nest <- wm_df %>%
  mutate(text = str_to_lower(text)) %>% 
  mutate(positionality = "white male") %>%
  mutate(text = str_squish(text)) %>%
  mutate(text = removeNumbers(text))
#unnest
wm_unnest_auth <- wm_nest %>% 
  unnest_tokens(output = "word", input = "text") %>%
  mutate(positionality_choice = "Gender and Race") %>%
  mutate(positionality_of_df = "White Male")
#remove stop words and punctuation
wm <- anti_join(wm_unnest, stop_words, by = c("word" = "word"), .keep_all = true)
wm <- wm %>% mutate(word = removePunctuation(word))
#lemmatization
wm_lem <- wm$word %>% lemmatize_words()
wm_lem <- as.data.frame(wm_lem) 
wm_lem <- wm_lem %>%
  mutate(id = row_number()) 
wm_lem_count <- wm %>% mutate(id = row_number())
wm <- full_join(wm_lem, wm_lem_count, by = c("id" = "id"))
wm_auth <- wm %>% select(-id) %>% rename(lem = "wm_lem") %>%
  mutate(positionality_choice = "Gender and Race")

#white female 
#read
wf_df <- readtext("data/white_fem_auth", cache=FALSE)
#remove .txt
wf_df$doc_id<-gsub(".txt", "", paste(wf_df$doc_id))
#still nested for +-10
wf_nest <- wf_df %>%
  mutate(text = str_to_lower(text)) %>% 
  mutate(positionality = "white fem") %>%
  mutate(text = str_squish(text)) %>%
  mutate(text = removeNumbers(text))
#unnest
wf_unnest_auth <- wf_nest %>% 
  unnest_tokens(output = "word", input = "text") %>%
  mutate(positionality_choice = "Gender and Race") %>%
  mutate(positionality_of_df = "White Female")
#remove stop words
wf <- anti_join(wf_unnest, stop_words, by = c("word" = "word"), .keep_all = true)
wf <- wf %>% mutate(word = removePunctuation(word))
#lemmatization
wf_lem <- wf$word %>% lemmatize_words()
wf_lem <- as.data.frame(wf_lem) 
wf_lem <- wf_lem %>%
  mutate(id = row_number()) 
wf_lem_count <- wf %>% mutate(id = row_number())
wf <- full_join(wf_lem, wf_lem_count, by = c("id" = "id"))
wf_auth <- wf %>% select(-id) %>% rename(lem = "wf_lem") %>%
  mutate(positionality_choice = "Gender and Race")

#black female 
#read
bf_df <- readtext("data/black_fem_auth", cache=FALSE)
#remove .txt
bf_df$doc_id<-gsub(".txt", "", paste(bf_df$doc_id))
#still nested for +-10
bf_nest <- bf_df %>%
  mutate(text = str_to_lower(text)) %>% 
  mutate(positionality = "black fem") %>%
  mutate(text = str_squish(text)) %>%
  mutate(text = removeNumbers(text))
#unnest
bf_unnest_auth <- bf_nest %>% 
  unnest_tokens(output = "word", input = "text") %>%
  mutate(positionality_choice = "Gender and Race")  %>%
  mutate(positionality_of_df = "Black Female")
#remove stop words
bf <- anti_join(bf_unnest, stop_words, by = c("word" = "word"), .keep_all = true)
bf <- bf %>% mutate(word = removePunctuation(word))
#lemmatization
bf_lem <- bf$word %>% lemmatize_words()
bf_lem <- as.data.frame(bf_lem) 
bf_lem <- bf_lem %>%
  mutate(id = row_number()) 
bf_lem_count <- bf %>% mutate(id = row_number())
bf <- full_join(bf_lem, bf_lem_count, by = c("id" = "id"))
bf_auth <- bf %>% select(-id) %>% rename(lem = "bf_lem") %>%
  mutate(positionality_choice = "Gender and Race")

#remove unneeded
rm(wm_df, wm_lem_count, wf_df, wf_lem_count, bf_df, bf_lem_count, wm_nest, wf_nest, bf_nest)


# write.csv(wm_unnest_auth, "data/wm_unnest_auth")
# write.csv(wf_unnest_auth, "data/wf_unnest_auth")
# write.csv(bf_unnest_auth, "data/bf_unnest_auth")
# write.csv(wm_auth, "data/wm_auth")
# write.csv(wf_auth, "data/wf_auth")
# write.csv(bf_auth, "data/bf_auth")
```

BY PROTAG POSITIONALITY
```{r}
#white male 
#read
wm_df <- readtext("data/white_male_char", cache=FALSE)
#remove .txt
wm_df$doc_id<-gsub(".txt", "", paste(wm_df$doc_id))
#still nested for +-10
wm_nest <- wm_df %>%
  mutate(text = str_to_lower(text)) %>% 
  mutate(positionality = "white male") %>%
  mutate(text = str_squish(text)) %>%
  mutate(text = removeNumbers(text))
#unnest
wm_unnest_char <- wm_nest %>% 
  unnest_tokens(output = "word", input = "text") %>%
  mutate(positionality_choice = "Gender and Race")  %>%
  mutate(positionality_of_df = "White Male")
#remove stop words and punctuation
wm <- anti_join(wm_unnest, stop_words, by = c("word" = "word"), .keep_all = true)
wm <- wm %>% mutate(word = removePunctuation(word))
#lemmatization
wm_lem <- wm$word %>% lemmatize_words()
wm_lem <- as.data.frame(wm_lem) 
wm_lem <- wm_lem %>%
  mutate(id = row_number()) 
wm_lem_count <- wm %>% mutate(id = row_number())
wm <- full_join(wm_lem, wm_lem_count, by = c("id" = "id"))
wm_char <- wm %>% select(-id) %>% rename(lem = "wm_lem") %>%
  mutate(positionality_choice = "Gender and Race")

#white female 
#read
wf_df <- readtext("data/white_fem_char", cache=FALSE)
#remove .txt
wf_df$doc_id<-gsub(".txt", "", paste(wf_df$doc_id))
#still nested for +-10
wf_nest <- wf_df %>%
  mutate(text = str_to_lower(text)) %>% 
  mutate(positionality = "white fem") %>%
  mutate(text = str_squish(text)) %>%
  mutate(text = removeNumbers(text))
#unnest
wf_unnest_char <- wf_nest %>% 
  unnest_tokens(output = "word", input = "text") %>%
  mutate(positionality_choice = "Gender and Race")  %>%
  mutate(positionality_of_df = "White Female")
#remove stop words
wf <- anti_join(wf_unnest, stop_words, by = c("word" = "word"), .keep_all = true)
wf <- wf %>% mutate(word = removePunctuation(word))
#lemmatization
wf_lem <- wf$word %>% lemmatize_words()
wf_lem <- as.data.frame(wf_lem) 
wf_lem <- wf_lem %>%
  mutate(id = row_number()) 
wf_lem_count <- wf %>% mutate(id = row_number())
wf <- full_join(wf_lem, wf_lem_count, by = c("id" = "id"))
wf_char <- wf %>% select(-id) %>% rename(lem = "wf_lem") %>%
  mutate(positionality_choice = "Gender and Race")

#black female 
#read
bf_df <- readtext("data/black_fem_char", cache=FALSE)
#remove .txt
bf_df$doc_id<-gsub(".txt", "", paste(bf_df$doc_id))
#still nested for +-10
bf_nest <- bf_df %>%
  mutate(text = str_to_lower(text)) %>% 
  mutate(positionality = "black fem") %>%
  mutate(text = str_squish(text)) %>%
  mutate(text = removeNumbers(text))
#unnest
bf_unnest_char <- bf_nest %>% 
  unnest_tokens(output = "word", input = "text") %>%
  mutate(positionality_choice = "Gender and Race")  %>%
  mutate(positionality_of_df = "Black Female")
#remove stop words
bf <- anti_join(bf_unnest, stop_words, by = c("word" = "word"), .keep_all = true)
bf <- bf %>% mutate(word = removePunctuation(word))
#lemmatization
bf_lem <- bf$word %>% lemmatize_words()
bf_lem <- as.data.frame(bf_lem) 
bf_lem <- bf_lem %>%
  mutate(id = row_number()) 
bf_lem_count <- bf %>% mutate(id = row_number())
bf <- full_join(bf_lem, bf_lem_count, by = c("id" = "id"))
bf_char <- bf %>% select(-id) %>% rename(lem = "bf_lem") %>%
  mutate(positionality_choice = "Gender and Race")

#remove unneeded
rm(wm_df, wm_lem_count, wf_df, wf_lem_count, bf_df, bf_lem_count, wm_nest, wf_nest, bf_nest)


# write.csv(wm_unnest, "data/wm_unnest_char")
# write.csv(wf_unnest, "data/wf_unnest_char")
# write.csv(bf_unnest, "data/bf_unnest_char")
# write.csv(wm, "data/wm_char")
# write.csv(wf, "data/wf_char")
# write.csv(bf, "data/bf_char")
```

BY GENDER PROTOG
```{r}
#male 
#read
wm <- readtext("data/white_male_char", cache=FALSE)
bm <- readtext("data/black_male_char", cache=FALSE)
male <- bind_rows(wm, bm)
#remove .txt
male$doc_id<-gsub(".txt", "", paste(male$doc_id))
#still nested for +-10
male_nest <- male %>%
  mutate(text = str_to_lower(text)) %>% 
  mutate(positionality = "male") %>%
  mutate(text = str_squish(text)) %>%
  mutate(text = removeNumbers(text))
#unnest
male_unnest_char <- male_nest %>% 
  unnest_tokens(output = "word", input = "text") %>%
  mutate(positionality_choice = "Gender")  %>%
  mutate(positionality_of_df = "Male")
#remove stop words and punctuation
male <- anti_join(male_unnest, stop_words, by = c("word" = "word"), .keep_all = true)
male <- male %>% mutate(word = removePunctuation(word))
#lemmatization
male_lem <- male$word %>% lemmatize_words()
male_lem <- as.data.frame(male_lem) 
male_lem <- male_lem %>%
  mutate(id = row_number()) 
male_lem_count <- male %>% mutate(id = row_number())
male <- full_join(male_lem, male_lem_count, by = c("id" = "id"))
male_char <- male %>% select(-id) %>% rename(lem = "male_lem") %>%
  mutate(positionality_choice = "Gender")

#female 
wf <- readtext("data/white_fem_char", cache=FALSE)
bf <- readtext("data/black_fem_char", cache=FALSE)
fem <- bind_rows(wf, bf)
#remove .txt
fem$doc_id<-gsub(".txt", "", paste(fem$doc_id))
#still nested for +-10
fem_nest <- fem %>%
  mutate(text = str_to_lower(text)) %>% 
  mutate(positionality = "female") %>%
  mutate(text = str_squish(text)) %>%
  mutate(text = removeNumbers(text))
#unnest
fem_unnest_char <- fem_nest %>% 
  unnest_tokens(output = "word", input = "text") %>%
  mutate(positionality_choice = "Gender")  %>%
  mutate(positionality_of_df = "Female")
#remove stop words and punctuation
fem <- anti_join(fem_unnest, stop_words, by = c("word" = "word"), .keep_all = true)
fem <- fem %>% mutate(word = removePunctuation(word))
#lemmatization
fem_lem <- fem$word %>% lemmatize_words()
fem_lem <- as.data.frame(fem_lem) 
fem_lem <- fem_lem %>%
  mutate(id = row_number()) 
fem_lem_count <- fem %>% mutate(id = row_number())
fem <- full_join(fem_lem, fem_lem_count, by = c("id" = "id"))
fem_char <- fem %>% select(-id) %>% rename(lem = "fem_lem") %>%
  mutate(positionality_choice = "Gender")

#remove unneeded
rm(wm, male_lem_count, wf, fem_lem_count, bf, bm, male_nest, fem_nest)


# write.csv(fem_unnest_char, "data/fem_unnest_char")
# write.csv(male_unnest_char, "data/male_unnest_char")
# write.csv(fem_char, "data/fem_char")
# write.csv(male_char, "data/male_char")
```

BY GENDER AUTH
```{r}
#male 
#read
wm <- readtext("data/white_male_auth", cache=FALSE)
bm <- readtext("data/black_male_auth", cache=FALSE)
male <- bind_rows(wm, bm)
#remove .txt
male$doc_id<-gsub(".txt", "", paste(male$doc_id))
#still nested for +-10
male_nest <- male %>%
  mutate(text = str_to_lower(text)) %>% 
  mutate(positionality = "male") %>%
  mutate(text = str_squish(text)) %>%
  mutate(text = removeNumbers(text))
#unnest
male_unnest_auth <- male_nest %>% 
  unnest_tokens(output = "word", input = "text") %>%
  mutate(positionality_choice = "Gender")  %>%
  mutate(positionality_of_df = "Male")
#remove stop words and punctuation
male <- anti_join(male_unnest, stop_words, by = c("word" = "word"), .keep_all = true)
male <- male %>% mutate(word = removePunctuation(word))
#lemmatization
male_lem <- male$word %>% lemmatize_words()
male_lem <- as.data.frame(male_lem) 
male_lem <- male_lem %>%
  mutate(id = row_number()) 
male_lem_count <- male %>% mutate(id = row_number())
male <- full_join(male_lem, male_lem_count, by = c("id" = "id"))
male_auth <- male %>% select(-id) %>% rename(lem = "male_lem") %>%
  mutate(positionality_choice = "Gender")

#female 
wf <- readtext("data/white_fem_auth", cache=FALSE)
bf <- readtext("data/black_fem_auth", cache=FALSE)
fem <- bind_rows(wf, bf)
#remove .txt
fem$doc_id<-gsub(".txt", "", paste(fem$doc_id))
#still nested for +-10
fem_nest <- fem %>%
  mutate(text = str_to_lower(text)) %>% 
  mutate(positionality = "female") %>%
  mutate(text = str_squish(text)) %>%
  mutate(text = removeNumbers(text))
#unnest
fem_unnest_auth <- fem_nest %>% 
  unnest_tokens(output = "word", input = "text") %>%
  mutate(positionality_choice = "Gender")  %>%
  mutate(positionality_of_df = "Female")
#remove stop words and punctuation
fem <- anti_join(fem_unnest, stop_words, by = c("word" = "word"), .keep_all = true)
fem <- fem %>% mutate(word = removePunctuation(word))
#lemmatization
fem_lem <- fem$word %>% lemmatize_words()
fem_lem <- as.data.frame(fem_lem) 
fem_lem <- fem_lem %>%
  mutate(id = row_number()) 
fem_lem_count <- fem %>% mutate(id = row_number())
fem <- full_join(fem_lem, fem_lem_count, by = c("id" = "id"))
fem_auth <- fem %>% select(-id) %>% rename(lem = "fem_lem") %>%
  mutate(positionality_choice = "Gender")

#remove unneeded
rm(wm, male_lem_count, wf, fem_lem_count, bf, bm, fem_nest, male_nest)


# write.csv(fem_unnest_auth, "data/fem_unnest_auth")
# write.csv(male_unnest_auth, "data/male_unnest_auth")
# write.csv(fem_auth, "data/fem_auth")
# write.csv(male_auth, "data/male_auth")
```

RACE
```{r}
#white 
#read
w_df <- readtext("data/white", cache=FALSE)
#remove .txt
w_df$doc_id<-gsub(".txt", "", paste(w_df$doc_id))
#still nested for +-10
w_nest <- w_df %>%
  mutate(text = str_to_lower(text)) %>% 
  mutate(positionality = "white") %>%
  mutate(text = str_squish(text)) %>%
  mutate(text = removeNumbers(text))
#unnest
w_unnest <- w_nest %>% 
  unnest_tokens(output = "word", input = "text")   %>%
  mutate(positionality_choice = "Race") %>%
  mutate(positionality_of_df = "White")
#remove stop words and punctuation
w <- anti_join(w_unnest, stop_words, by = c("word" = "word"), .keep_all = true)
w <- w %>% mutate(word = removePunctuation(word))
#lemmatization
w_lem <- w$word %>% lemmatize_words()
w_lem <- as.data.frame(w_lem) 
w_lem <- w_lem %>%
  mutate(id = row_number()) 
w_lem_count <- w %>% mutate(id = row_number())
w <- full_join(w_lem, w_lem_count, by = c("id" = "id"))
w <- w %>% select(-id) %>% rename(lem = "w_lem") %>%
  mutate(positionality_choice = "Race")

#black  
#read
b_df <- readtext("data/black", cache=FALSE)
#remove .txt
b_df$doc_id<-gsub(".txt", "", paste(b_df$doc_id))
#still nested for +-10
b_nest <- b_df %>%
  mutate(text = str_to_lower(text)) %>% 
  mutate(positionality = "black") %>%
  mutate(text = str_squish(text)) %>%
  mutate(text = removeNumbers(text))
#unnest
b_unnest <- b_nest %>% 
  unnest_tokens(output = "word", input = "text")%>%
  mutate(positionality_choice = "Race") %>%
  mutate(positionality_of_df = "Black")
#remove stop words
b <- anti_join(b_unnest, stop_words, by = c("word" = "word"), .keep_all = true)
b <- b %>% mutate(word = removePunctuation(word))
#lemmatization
b_lem <- b$word %>% lemmatize_words()
b_lem <- as.data.frame(b_lem) 
b_lem <- b_lem %>%
  mutate(id = row_number()) 
b_lem_count <- b %>% mutate(id = row_number())
b <- full_join(b_lem, b_lem_count, by = c("id" = "id"))
b <- b %>% select(-id) %>% rename(lem = "b_lem") %>%
  mutate(positionality_choice = "Race")

#remove unneeded
rm(w_df, w_lem_count, b_df, b_lem_count, b_nest, w_nest)


# write.csv(w_unnest, "data/w_unnest")
# write.csv(b_unnest, "data/b_unnest")
# write.csv(w, "data/w")
# write.csv(b, "data/b")
```



DFs
```{r}
#all novels
all_novels_auth_both <- bind_rows(wm_auth, wf_auth, bf_auth) %>%
  group_by(doc_id) %>%
  mutate(id = row_number()) %>%
  mutate(tot_words = n()) %>%
  ungroup() %>%
  mutate(positionality_choice = "Gender and Race")

all_novels_auth_gender <- bind_rows(fem_auth, male_auth) %>%
  group_by(doc_id) %>%
  mutate(id = row_number()) %>%
  mutate(tot_words = n()) %>%
  ungroup() %>%
  mutate(positionality_choice = "Gender")


all_novels_race <- bind_rows(w, b) %>%
  group_by(doc_id) %>%
  mutate(id = row_number()) %>%
  mutate(tot_words = n()) %>%
  ungroup() %>%
  mutate(positionality_choice = "Race")


all_novels_char_both <- bind_rows(wm_char, wf_char, bf_char) %>%
  group_by(doc_id) %>%
  mutate(id = row_number()) %>%
  mutate(tot_words = n()) %>%
  ungroup() %>%
  mutate(positionality_choice = "Gender and Race")

all_novels_char_gender <- bind_rows(fem_char, male_char) %>%
  group_by(doc_id) %>%
  mutate(id = row_number()) %>%
  mutate(tot_words = n()) %>%
  ungroup() %>%
  mutate(positionality_choice = "Gender")

all_novels_auth <- bind_rows(all_novels_auth_both, all_novels_auth_gender, all_novels_race)
write.csv(all_novels_auth, "data/all_novels_auth")

all_novels_char <- bind_rows(all_novels_char_both, all_novels_char_gender, all_novels_race)
write.csv(all_novels_char, "data/all_novels_char")

novels_with_stop_auth <- bind_rows(w_unnest, b_unnest, fem_unnest_auth, male_unnest_auth, wm_unnest_auth, wf_unnest_auth, bf_unnest_auth) %>%
  mutate(choice = "auth")
novels_with_stop_char <- bind_rows(w_unnest, b_unnest, fem_unnest_char, male_unnest_char, wm_unnest_char, wf_unnest_char, bf_unnest_char) %>%
  mutate(choice = "char")

novels_with_stop <- bind_rows(novels_with_stop_auth, novels_with_stop_char)
```


VISUALIZATIONS !!!
```{r}
#Most common words used
#AUTH
top_10_words_wm_auth <- 
  wm_auth %>%
    count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
    filter(book_count>=8) %>% #tried 8 and keeps the same :)
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Gender and Race") %>%
  mutate(positionality_of_df = "White Male")

top_10_words_wf_auth <- 
  wf_auth %>%
    count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
    filter(book_count>=8) %>% #tried 8 and keeps the same :)
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Gender and Race") %>%
  mutate(positionality_of_df = "White Female")

top_10_words_bf_auth <- 
  bf_auth %>%
    count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
    filter(book_count>=8) %>% #tried 8 and keeps the same :)
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Gender and Race") %>%
  mutate(positionality_of_df = "Black Female")

top_10_words_fem_auth <- 
  fem_auth %>%
    count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
    filter(book_count>=8) %>% #tried 8 and keeps the same :)
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Gender") %>%
  mutate(positionality_of_df = "Female")

top_10_words_male_auth <- 
  male_auth %>%
    count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
    filter(book_count>=8) %>% #tried 8 and keeps the same :)
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Gender") %>%
  mutate(positionality_of_df = "Male")

top_10_words_white <- 
  w %>%
    count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
    filter(book_count>=8) %>% #tried 8 and keeps the same :)
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Race") %>%
  mutate(positionality_of_df = "White")

top_10_words_black <- 
  b %>%
    count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
    filter(book_count>=8) %>% #tried 8 and keeps the same :)
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Race") %>%
  mutate(positionality_of_df = "Black")

#CHAR

top_10_words_wm_char <- 
  wm_char %>%
    count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
    filter(book_count>=8) %>% #tried 8 and keeps the same :)
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Gender and Race") %>%
  mutate(positionality_of_df = "White Male")

top_10_words_wf_char <- 
  wf_char %>%
    count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
    filter(book_count>=8) %>% #tried 8 and keeps the same :)
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Gender and Race") %>%
  mutate(positionality_of_df = "White Female")

top_10_words_bf_char <- 
  bf_char %>%
    count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
    filter(book_count>=8) %>% #tried 8 and keeps the same :)
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Gender and Race") %>%
  mutate(positionality_of_df = "Black Female")

top_10_words_fem_char <- 
  fem_char %>%
    count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
    filter(book_count>=8) %>% #tried 8 and keeps the same :)
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Gender") %>%
  mutate(positionality_of_df = "Female")

top_10_words_male_char <- 
  male_char %>%
    count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
    filter(book_count>=8) %>% #tried 8 and keeps the same :)
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Gender") %>%
  mutate(positionality_of_df = "Male")

#CREATE DFS
top_10_words_auth <- bind_rows(top_10_words_wm_auth, top_10_words_wf_auth, top_10_words_bf_auth, top_10_words_fem_auth, top_10_words_male_auth, top_10_words_white, top_10_words_black) %>%
  mutate(choice = "auth")


top_10_words_char <- bind_rows(top_10_words_wm_char, top_10_words_wf_char, top_10_words_bf_char, top_10_words_fem_char, top_10_words_male_char, top_10_words_white, top_10_words_black) %>%
  mutate(choice = "char")

top_10_words <- bind_rows(top_10_words_auth, top_10_words_char)

write.csv(top_10_words, "data/top_10_words")
```

```{r}
#Sentiment score per novel
auth_afinn <-
    all_novels_auth_both %>% 
   inner_join(get_sentiments("afinn")) %>% 
   group_by(doc_id, positionality, tot_words) %>% 
   summarise(sentiment = sum(value)) %>% 
  mutate(perc = sentiment/tot_words) %>%
   mutate(method = "AFINN") %>%
  mutate(positionality_choice = "Gender and Race")


race_afinn <- 
    all_novels_race %>% 
   inner_join(get_sentiments("afinn")) %>% 
   group_by(doc_id, positionality, tot_words) %>% 
   summarise(sentiment = sum(value)) %>% 
  mutate(perc = sentiment/tot_words) %>%
   mutate(method = "AFINN") %>%
  mutate(positionality_choice = "Race")


auth_gender_afinn <-
  all_novels_auth_gender %>% 
   inner_join(get_sentiments("afinn")) %>% 
   group_by(doc_id, positionality, tot_words) %>% 
   summarise(sentiment = sum(value)) %>% 
  mutate(perc = sentiment/tot_words) %>%
   mutate(method = "AFINN") %>%
  mutate(positionality_choice = "Gender")


   #bing and nrc
bing_and_nrc_auth <- 
  bind_rows(
  all_novels_auth_both %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  all_novels_auth_both %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative")) 
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, doc_id, sentiment, positionality, tot_words) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  mutate(perc = sentiment / tot_words)%>%
  mutate(positionality_choice = "Gender and Race")

bing_and_nrc_race <-
                bind_rows(
  all_novels_race %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  all_novels_race %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, doc_id, sentiment, positionality, tot_words) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  mutate(perc = sentiment / tot_words) %>%
  mutate(positionality_choice = "Race") 


bing_and_nrc_auth_gender <-
                bind_rows(
  all_novels_auth_gender %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  all_novels_auth_gender %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, doc_id, sentiment, positionality, tot_words) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  mutate(perc = sentiment / tot_words) %>%
  mutate(positionality_choice = "Gender")

afinn_auth <- bind_rows(auth_afinn, race_afinn, auth_gender_afinn) 

bing_and_nrc_auth <- bind_rows(bing_and_nrc_auth, bing_and_nrc_race, bing_and_nrc_auth_gender) 

sent_per_auth <- bind_rows(afinn_auth, bing_and_nrc_auth) %>%
  mutate(choice = "auth")


#-----#
  
char_afinn <-
    all_novels_char_both %>% 
   inner_join(get_sentiments("afinn")) %>% 
   group_by(doc_id, positionality, tot_words) %>% 
   summarise(sentiment = sum(value)) %>% 
  mutate(perc = sentiment/tot_words) %>%
   mutate(method = "AFINN") %>%
  mutate(positionality_choice = "Gender and Race")


char_gender_afinn <-
  all_novels_char_gender %>% 
   inner_join(get_sentiments("afinn")) %>% 
   group_by(doc_id, positionality, tot_words) %>% 
   summarise(sentiment = sum(value)) %>% 
  mutate(perc = sentiment/tot_words) %>%
   mutate(method = "AFINN") %>%
  mutate(positionality_choice = "Gender")


   #bing and nrc
bing_and_nrc_char <- 
  bind_rows(
  all_novels_char_both %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  all_novels_char_both %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative")) 
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, doc_id, sentiment, positionality, tot_words) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  mutate(perc = sentiment / tot_words)%>%
  mutate(positionality_choice = "Gender and Race")



bing_and_nrc_char_gender <-
                bind_rows(
  all_novels_char_gender %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  all_novels_char_gender %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, doc_id, sentiment, positionality, tot_words) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  mutate(perc = sentiment / tot_words) %>%
  mutate(positionality_choice = "Gender")

afinn_char <- bind_rows(char_afinn, race_afinn, char_gender_afinn) 

bing_and_nrc_char <- bind_rows(bing_and_nrc_char, bing_and_nrc_race, bing_and_nrc_char_gender) 

sent_per_char <- bind_rows(afinn_char, bing_and_nrc_char) %>%
  mutate(choice = "char")

sent_per <- bind_rows(sent_per_auth, sent_per_char)

write.csv(sent_per, "data/sent_per")
```

MENTAL HEALTH
```{r}
MH <- tibble(word=c("insane", "lobotomy", "hysteria", "electroshock", "lunatic", "psychotic", "nuts", "crazy", "deranged", "demented", "asylum", "loony", "retarded", "retard", "freak", "madness", "hydrotherapy", "delirious", "sociopath", "sociopathic", "disturbed", "psycho", "mad", "weird", "spastic", "halfwit", "meltdown", "outcast", "nutty", "killyourself", "killmyself", "killherself", "killhimself", "perverted", "spaz", "strange", "twisted", "weirdo", "psychopath", "psychopathic", "institutionalized", "therapy", "therapist", "psychiatrist", "psychiatric", "medication", "suicidal", "suicide", "mentallyill", "mentalillness", "diagnosis", "depression", "depressed", "anxiety", "anxieties", "psychosis", "dissociate", "dissociated", "dissociation", "intrusivethought", "intrusivethoughts", "thoughtspiral", "sleepparalysis", "multiplepersonality", "splitpersonality", "dissociative", "bipolar", "bpd", "borderlinepersonality", "schizophrenia", "schizophrenic", "ptsd", "dsm", "mentaldisorder", "mentalhealth", "anorexia", "bulimia", "bingeeating", "manic", "autism", "hallucinations", "hallucination", "ocd", "psychoanalysis", "trigger", "panicattack", "panicdisorder", "coping", "compulsion", "antisocialpersonality",  "selfharm", "selfmutilation", "selfinjury", "selfharmers", "selfharmer", "straitjacket", "mentalhospital", "mentalinstitution", "depersonalization", "obsessivecompulsive", "psychiatry", "delirium", "mania", "melancholia", "monomania", "traumatic", "trauma", "cutter", "cutting", "schizo", "delusional", "alters", "shellshock", "insomnia", "neurosis", "neurotic", "delusions", "survivorsguilt", "mentalcase", "agoraphobia", "mentallychallenged", "abnormal", "mentalpatient"))

neg_mh <- tibble(word=c("insane", "lobotomy", "hysteria", "electroshock", "lunatic", "psychotic", "nuts", "crazy", "deranged", "demented", "asylum", "loony", "retarded", "retard", "freak", "madness", "hydrotherapy", "delirious", "sociopath", "sociopathic", "disturbed", "psycho", "mad", "weird", "spastic", "halfwit", "meltdown", "outcast", "nutty", "killyourself", "killmyself", "killherself", "killhimself", "perverted", "spaz", "strange", "twisted", "weirdo", "psychopath", "psychopathic", "cutter", "cutting", "schizo", "delusional", "shellshock", "neurosis", "neurotic", "mentalcase", "multiplepersonality", "splitpersonality", "abnormal"))

pos_mh <- tibble(word=c("institutionalized", "therapy", "therapist", "psychiatrist", "psychiatric", "medication", "suicidal", "suicide", "mentallyill", "mentalillness", "mentalhealth", "diagnosis", "depression", "depressed", "anxiety", "anxieties", "psychosis", "dissociate", "dissociated", "dissociation", "intrusivethoughts", "intrusivethought", "thoughtspiral", "sleepparalysis", "dissociative", "bipolar", "bpd", "borderlinepersonality", "schizophrenia", "schizophrenic", "ptsd", "dsm", "mentaldisorder", "anorexia", "bulimia", "bingeeating", "manic", "autism", "hallucinations", "hallucination", "ocd", "psychoanalysis", "trigger", "panicattack", "panicdisorder", "coping", "compulsion", "antisocialpersonality",  "selfharm", "selfmutilation", "selfinjury", "selfharmers", "selfharmer", "straitjacket", "mentalinstitution", "mentalhospital", "depersonalization", "obsessivecompulsive", "psychiatry", "delirium", "mania", "melancholia", "monomania", "traumatic", "trauma", "alters", "insomnia", "delusions", "survivors guilt", "agoraphobia", "mentalpatient"))
```

Fix 2 word mh terms
```{r}
combo <- novels_with_stop %>%
  mutate(next_word = lead(word)) %>%
  ungroup() %>%
  mutate(word =
           case_when(word=="mentally" & next_word=="challenged" ~ "mentallychallenged",
                     word=="kill" & next_word=="yourself" ~ "killyourself", 
                     word=="kill" & next_word=="myself" ~ "killmyself", 
                     word=="kill" & next_word=="herself" ~ "killherself", 
                     word=="kill" & next_word=="himself" ~ "killhimself",
                     word=="multiple" & next_word=="personality" ~ "multiplepersonality",
                     word=="mentally" & next_word=="ill" ~ "mentallyill",
                     word=="mental" & next_word=="illness" ~ "mentalillness",
                     word=="intrusive" & next_word=="thoughts" ~ "intrusivethoughts",
                     word=="intrusive" & next_word=="thought" ~ "intrusivethought",
                     word=="thought" & next_word=="spiral" ~ "thoughtspiral",
                     word=="multiple" & next_word=="personality" ~ "multiplepersonality",
                     word=="split" & next_word=="personality" ~ "splitpersonality",
                     word=="borderline" & next_word=="personality" ~ "borderlinepersonality",
                     word=="binge" & next_word=="eating" ~ "bingeeating",
                     word=="self" & next_word=="harm" ~ "selfharm",
                     word=="self" & next_word=="mutilation" ~ "selfmutilation",
                     word=="self" & next_word=="injury" ~ "selfinjury",
                     word=="self" & next_word=="harmers" ~ "selfharmers",
                     word=="self" & next_word=="harmer" ~ "selfharmer",
                     word=="mental" & next_word=="hospital" ~ "mentalhospital",
                     word=="mental" & next_word=="institution" ~ "mentalinstitution",
                     word=="obsessive" & next_word=="compulsive" ~ "obsessivecompulsive", 
                     word=="shell" & next_word=="shock" ~ "shellshock",
                     word=="survivors" & next_word=="guilt" ~ "survivorsguilt",
                     word=="mental" & next_word=="case" ~ "mentalcase",
                     word=="sleep" & next_word=="paralysis" ~ "sleepparalysis",
                     word=="panic" & next_word=="attack" ~ "panicattack",
                     word=="panic" & next_word=="disorder" ~ "panicdisorder",
                     word=="antisocial" & next_word=="personality" ~ "antisocialpersonality",
                     word=="mental" & next_word=="patient" ~ "mentalpatient",
                     TRUE ~ word)) %>% #remove next word
    select(!next_word)

combo_with_stop <- combo %>% mutate(word = removePunctuation(word)) %>%
    group_by(doc_id) %>%
    mutate(id = row_number()) %>%
    mutate(tot_words = n()) %>%
    ungroup()

   #remove stop words
combo_remove_stop <- anti_join(combo, stop_words, by = c("word" = "word"), .keep_all = true)
combo_remove_stop <- combo_remove_stop %>% mutate(word = removePunctuation(word)) %>%
    group_by(doc_id) %>%
    mutate(id = row_number()) %>%
    mutate(tot_words = n()) %>%
    ungroup()

MHcount <- combo_remove_stop %>%
  inner_join(MH) %>%
  group_by(doc_id) %>%
  summarise(positionality_choice = positionality_choice, positionality_of_df = positionality_of_df,
            choice = choice,
            mh_tot = n(), mh_perc = mh_tot / tot_words) %>%
  ungroup() %>% distinct()

write.csv(MHcount, "data/MHcount")

write.csv(combo_remove_stop, "data/combo_remove_stop")
write.csv(combo_with_stop, "data/combo_with_stop")
```


```{r}
#Most common mental health words

top_10_mh_wm_auth <- combo_remove_stop %>%
  filter(positionality_of_df == "White Male") %>%
  filter(choice == "auth") %>%
   count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
     ungroup() %>%
    filter(book_count>=2) %>%
  inner_join(MH) %>%
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Gender and Race") %>%
  mutate(positionality_of_df = "White Male")

top_10_mh_wf_auth <- combo_remove_stop %>%
  filter(positionality_of_df == "White Female") %>%
  filter(choice == "auth") %>%
   count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
     ungroup() %>%
    filter(book_count>=2) %>%
  inner_join(MH) %>%
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq))%>%
  mutate(positionality_choice = "Gender and Race") %>%
  mutate(positionality_of_df = "White Female")

top_10_mh_bf_auth <- combo_remove_stop %>%
  filter(positionality_of_df == "Black Female") %>%
  filter(choice == "auth") %>%
   count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
     ungroup() %>%
    filter(book_count>=2) %>%
  inner_join(MH) %>%
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq))%>%
  mutate(positionality_choice = "Gender and Race") %>%
  mutate(positionality_of_df = "Black Female")

top_10_mh_male_auth <- combo_remove_stop %>%
  filter(positionality_of_df == "Male") %>%
  filter(choice == "auth") %>%
   count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
     ungroup() %>%
    filter(book_count>=2) %>%
  inner_join(MH) %>%
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq))%>%
  mutate(positionality_choice = "Gender") %>%
  mutate(positionality_of_df = "Male")

top_10_mh_fem_auth <- combo_remove_stop %>%
  filter(positionality_of_df == "Female") %>%
  filter(choice == "auth") %>%
   count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
     ungroup() %>%
    filter(book_count>=2) %>%
  inner_join(MH) %>%
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Gender") %>%
  mutate(positionality_of_df = "Female")

top_10_mh_w <- combo_remove_stop %>%
  filter(positionality_of_df == "White") %>%
  filter(choice == "auth") %>%
   count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
     ungroup() %>%
    filter(book_count>=2) %>%
  inner_join(MH) %>%
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Race") %>%
  mutate(positionality_of_df = "White")

top_10_mh_b <- combo_remove_stop %>%
  filter(positionality_of_df == "Black") %>%
  filter(choice == "auth") %>%
   count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
     ungroup() %>%
    filter(book_count>=2) %>%
  inner_join(MH) %>%
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Race") %>%
  mutate(positionality_of_df = "Black")

top_10_mh_wm_char <- combo_remove_stop %>%
  filter(positionality_of_df == "White Male") %>%
  filter(choice == "char") %>%
   count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
     ungroup() %>%
    filter(book_count>=2) %>%
  inner_join(MH) %>%
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Gender and Race") %>%
  mutate(positionality_of_df = "White Male")

top_10_mh_wf_char <- combo_remove_stop %>%
  filter(positionality_of_df == "White Female") %>%
  filter(choice == "char") %>%
   count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
     ungroup() %>%
    filter(book_count>=2) %>%
  inner_join(MH) %>%
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Gender and Race") %>%
  mutate(positionality_of_df = "White Female")

top_10_mh_bf_char <- combo_remove_stop %>%
  filter(positionality_of_df == "Black Female") %>%
  filter(choice == "char") %>%
   count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
     ungroup() %>%
    filter(book_count>=2) %>%
  inner_join(MH) %>%
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Gender and Race") %>%
  mutate(positionality_of_df = "Black Female")

top_10_mh_fem_char <- combo_remove_stop %>%
  filter(positionality_of_df == "Female") %>%
  filter(choice == "char") %>%
   count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
     ungroup() %>%
    filter(book_count>=2) %>%
  inner_join(MH) %>%
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq)) %>%
  mutate(positionality_choice = "Gender") %>%
  mutate(positionality_of_df = "Female")

top_10_mh_male_char <- combo_remove_stop %>%
  filter(positionality_of_df == "Male") %>%
  filter(choice == "char") %>%
   count(doc_id, word) %>%
  group_by(doc_id) %>%
  mutate(tot_words = sum(n)) %>%
  ungroup() %>%
  group_by(word) %>%
    mutate(book_count = n()) %>%
     ungroup() %>%
    filter(book_count>=2) %>%
  inner_join(MH) %>%
    mutate(freq = n / tot_words) %>%
    group_by(word) %>%
    summarize(tot_freq = sum(freq)) %>%
  slice_max(tot_freq, n=10) %>%
  mutate(word = fct_reorder(.f = word, .x = tot_freq))%>%
  mutate(positionality_choice = "Gender") %>%
  mutate(positionality_of_df = "Male")

#CREATE DFS
top_10_mh_auth <- bind_rows(top_10_mh_wm_auth, top_10_mh_wf_auth, top_10_mh_bf_auth, top_10_mh_fem_auth, top_10_mh_male_auth, top_10_mh_w, top_10_mh_b) %>%
  mutate(choice = "auth")


top_10_mh_char <- bind_rows(top_10_mh_wm_char, top_10_mh_wf_char, top_10_mh_bf_char, top_10_mh_fem_char, top_10_mh_male_char, top_10_mh_w, top_10_mh_b) %>%
  mutate(choice = "char")

top_10_mh <- bind_rows(top_10_mh_auth, top_10_mh_char)

write.csv(top_10_mh, "data/top_10_mh")
```

+- 10
```{r}
target10=c("insane", "lobotomy", "hysteria", "electroshock", "lunatic", "psychotic", "nuts", "crazy", "deranged", "demented", "asylum", "loony", "retarded", "retard", "freak", "madness", "hydrotherapy", "delirious", "sociopath", "sociopathic", "disturbed", "psycho", "mad", "weird", "spastic", "halfwit", "meltdown", "outcast", "nutty", "killyourself", "killmyself", "killherself", "killhimself", "perverted", "spaz", "strange", "twisted", "weirdo", "psychopath", "psychopathic", "institutionalized", "therapy", "therapist", "psychiatrist", "psychiatric", "medication", "suicidal", "suicide", "mentallyill", "mentalillness", "diagnosis", "depression", "depressed", "anxiety", "anxieties", "psychosis", "dissociate", "dissociated", "dissociation", "intrusivethought", "intrusivethoughts", "thoughtspiral", "sleepparalysis", "multiplepersonality", "splitpersonality", "dissociative", "bipolar", "bpd", "borderlinepersonality", "schizophrenia", "schizophrenic", "ptsd", "dsm", "mentaldisorder", "mentalhealth", "anorexia", "bulimia", "bingeeating", "manic", "autism", "hallucinations", "hallucination", "ocd", "psychoanalysis", "trigger", "panicattack", "panicdisorder", "coping", "compulsion", "antisocialpersonality",  "selfharm", "selfmutilation", "selfinjury", "selfharmers", "selfharmer", "straitjacket", "mentalhospital", "mentalinstitution", "depersonalization", "obsessivecompulsive", "psychiatry", "delirium", "mania", "melancholia", "monomania", "traumatic", "trauma", "cutter", "cutting", "schizo", "delusional", "alters", "shellshock", "insomnia", "neurosis", "neurotic", "delusions", "survivorsguilt", "mentalcase", "agoraphobia", "mentallychallenged", "abnormal", "mentalpatient")

combo_with_stop_gender_auth <- combo_with_stop %>% filter(choice == "auth") %>% filter(positionality_choice == "Gender")

combo_with_stop_both_auth <- combo_with_stop %>% filter(choice == "auth") %>% filter(positionality_choice == "Gender and Race")

combo_with_stop_both_char <- combo_with_stop %>% filter(choice == "char") %>% filter(positionality_choice == "Gender and Race")

combo_with_stop_gender_char <- combo_with_stop %>% filter(choice == "auth") %>% filter(positionality_choice == "Gender")

combo_with_stop_race <- combo_with_stop %>% filter(choice == "auth") %>% filter(positionality_choice == "Race")

#doc names and count
docs_gender_auth = unique(combo_with_stop_gender_auth$doc_id)
ndoc_gender_auth = length(unique(combo_with_stop_gender_auth$doc_id))
#for loop for 10 words around csvs
for (i in 1:ndoc_gender_auth){
  #full data
  test <- combo_with_stop_gender_auth %>%
  #subset data set into current doc
    filter(doc_id == docs_gender_auth[i]) %>%
  #build 10 word lead/lag
    mutate(ten_words = case_when(word %in% target10 ~ paste(
      lag(word, 10), lag(word, 9),lag(word, 8), lag(word, 7), lag(word, 6), lag(word, 5), lag(word, 4), lag(word, 3), lag(word, 2), lag(word), 
      lead(word), lead(word, 2), lead(word, 3), lead(word, 4), lead(word, 5), lead(word, 6), lead(word, 7), lead(word, 8), lead(word, 9), lead(word, 10)))) %>%
    drop_na(ten_words)
  #save to file
  filename = paste("10_words_gender_auth/", docs_gender_auth[i], "_10_words.csv", sep = "")
  write_csv(x = test, file = filename)
}

#sentiment
sentiment_around_mh_gender_auth <- readtext("10_words_gender_auth", cache=FALSE) %>%
  select(!doc_id) %>%
  rename(doc_id = "text") %>% 
  select(!word) %>%
  unnest_tokens(output = "word", input = "ten_words") %>%
   inner_join(get_sentiments("bing")) %>%
  count(doc_id, positionality_of_df, sentiment) %>%
  spread(sentiment, n, fill=0) %>%
  mutate(sentiment = positive - negative) %>%
  mutate(positionality_choice = "Gender")

#doc names and count
docs_gender_char = unique(combo_with_stop_gender_char$doc_id)
ndoc_gender_char = length(unique(combo_with_stop_gender_char$doc_id))
#for loop for 10 words around csvs
for (i in 1:ndoc_gender_char){
  #full data
  test <- combo_with_stop_gender_char %>%
  #subset data set into current doc
    filter(doc_id == docs_gender_char[i]) %>%
  #build 10 word lead/lag
    mutate(ten_words = case_when(word %in% target10 ~ paste(
      lag(word, 10), lag(word, 9),lag(word, 8), lag(word, 7), lag(word, 6), lag(word, 5), lag(word, 4), lag(word, 3), lag(word, 2), lag(word), 
      lead(word), lead(word, 2), lead(word, 3), lead(word, 4), lead(word, 5), lead(word, 6), lead(word, 7), lead(word, 8), lead(word, 9), lead(word, 10)))) %>%
    drop_na(ten_words)
  #save to file
  filename = paste("10_words_gender_char/", docs_gender_char[i], "_10_words.csv", sep = "")
  write_csv(x = test, file = filename)
}

#sentiment
sentiment_around_mh_gender_char <- readtext("10_words_gender_char", cache=FALSE) %>%
  select(!doc_id) %>%
  rename(doc_id = "text") %>% 
  select(!word) %>%
  unnest_tokens(output = "word", input = "ten_words") %>%
   inner_join(get_sentiments("bing")) %>%
  count(doc_id, positionality_of_df, sentiment) %>%
  spread(sentiment, n, fill=0) %>%
  mutate(sentiment = positive - negative) %>%
  mutate(positionality_choice = "Gender")

#doc names and count
docs_both_auth = unique(combo_with_stop_both_auth$doc_id)
ndoc_both_auth = length(unique(combo_with_stop_both_auth$doc_id))
#for loop for 10 words around csvs
for (i in 1:ndoc_both_auth){
  #full data
  test <- combo_with_stop_both_auth %>%
  #subset data set into current doc
    filter(doc_id == docs_both_auth[i]) %>%
  #build 10 word lead/lag
    mutate(ten_words = case_when(word %in% target10 ~ paste(
      lag(word, 10), lag(word, 9),lag(word, 8), lag(word, 7), lag(word, 6), lag(word, 5), lag(word, 4), lag(word, 3), lag(word, 2), lag(word), 
      lead(word), lead(word, 2), lead(word, 3), lead(word, 4), lead(word, 5), lead(word, 6), lead(word, 7), lead(word, 8), lead(word, 9), lead(word, 10)))) %>%
    drop_na(ten_words)
  #save to file
  filename = paste("10_words_both_auth/", docs_both_auth[i], "_10_words.csv", sep = "")
  write_csv(x = test, file = filename)
}

#sentiment
sentiment_around_mh_both_auth <- readtext("10_words_both_auth", cache=FALSE) %>%
  select(!doc_id) %>%
  rename(doc_id = "text") %>% 
  select(!word) %>%
  unnest_tokens(output = "word", input = "ten_words") %>%
   inner_join(get_sentiments("bing")) %>%
  count(doc_id, positionality_of_df, sentiment) %>%
  spread(sentiment, n, fill=0) %>%
  mutate(sentiment = positive - negative) %>%
  mutate(positionality_choice = "Gender and Race")

#doc names and count
docs_both_char = unique(combo_with_stop_both_char$doc_id)
ndoc_both_char = length(unique(combo_with_stop_both_char$doc_id))
#for loop for 10 words around csvs
for (i in 1:ndoc_both_char){
  #full data
  test <- combo_with_stop_both_char %>%
  #subset data set into current doc
    filter(doc_id == docs_both_char[i]) %>%
  #build 10 word lead/lag
    mutate(ten_words = case_when(word %in% target10 ~ paste(
      lag(word, 10), lag(word, 9),lag(word, 8), lag(word, 7), lag(word, 6), lag(word, 5), lag(word, 4), lag(word, 3), lag(word, 2), lag(word), 
      lead(word), lead(word, 2), lead(word, 3), lead(word, 4), lead(word, 5), lead(word, 6), lead(word, 7), lead(word, 8), lead(word, 9), lead(word, 10)))) %>%
    drop_na(ten_words)
  #save to file
  filename = paste("10_words_both_char/", docs_both_char[i], "_10_words.csv", sep = "")
  write_csv(x = test, file = filename)
}

#sentiment
sentiment_around_mh_both_char <- readtext("10_words_both_char", cache=FALSE) %>%
  select(!doc_id) %>%
  rename(doc_id = "text") %>% 
  select(!word) %>%
  unnest_tokens(output = "word", input = "ten_words") %>%
   inner_join(get_sentiments("bing")) %>%
  count(doc_id, positionality_of_df, sentiment) %>%
  spread(sentiment, n, fill=0) %>%
  mutate(sentiment = positive - negative) %>%
  mutate(positionality_choice = "Gender and Race")

#doc names and count
docs_race = unique(combo_with_stop_race$doc_id)
ndoc_race = length(unique(combo_with_stop_race$doc_id))
#for loop for 10 words around csvs
for (i in 1:ndoc_race){
  #full data
  test <- combo_with_stop_race %>%
  #subset data set into current doc
    filter(doc_id == docs_race[i]) %>%
  #build 10 word lead/lag
    mutate(ten_words = case_when(word %in% target10 ~ paste(
      lag(word, 10), lag(word, 9),lag(word, 8), lag(word, 7), lag(word, 6), lag(word, 5), lag(word, 4), lag(word, 3), lag(word, 2), lag(word), 
      lead(word), lead(word, 2), lead(word, 3), lead(word, 4), lead(word, 5), lead(word, 6), lead(word, 7), lead(word, 8), lead(word, 9), lead(word, 10)))) %>%
    drop_na(ten_words)
  #save to file
  filename = paste("10_words_race/", docs_race[i], "_10_words.csv", sep = "")
  write_csv(x = test, file = filename)
}

#sentiment
sentiment_around_mh_race <- readtext("10_words_race", cache=FALSE) %>%
  select(!doc_id) %>%
  rename(doc_id = "text") %>% 
  select(!word) %>%
  unnest_tokens(output = "word", input = "ten_words") %>%
   inner_join(get_sentiments("bing")) %>%
  count(doc_id, positionality_of_df, sentiment) %>%
  spread(sentiment, n, fill=0) %>%
  mutate(sentiment = positive - negative) %>%
  mutate(positionality_choice = "Race") 

sentiment_around_mh_auth <- bind_rows(sentiment_around_mh_gender_auth, sentiment_around_mh_both_auth, sentiment_around_mh_race) %>%
  mutate(choice = "auth")

sentiment_around_mh_char <- bind_rows(sentiment_around_mh_gender_char, sentiment_around_mh_both_char, sentiment_around_mh_race) %>%
  mutate(choice = "char")

sentiment_around_mh <- bind_rows(sentiment_around_mh_auth, sentiment_around_mh_char)

write.csv(sentiment_around_mh, "data/sentiment_around_mh")
```
